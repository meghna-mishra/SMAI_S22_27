{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kora"
      ],
      "metadata": {
        "id": "Dgt0403OT5bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "-Sfk9MfwRsmN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image, ImageOps, ImageEnhance\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import gc\n",
        "from torchvision import transforms\n",
        "import kora.install.rdkit\n",
        "from rdkit import Chem\n",
        "from typing import Optional\n",
        "import random\n",
        "from PIL import Image, ImageOps, ImageEnhance\n",
        "from typing import Union, List, Optional\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "id": "GLSeCp2WR_2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl"
      ],
      "metadata": {
        "id": "ZIEqwg4lUK_6"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 1e-4"
      ],
      "metadata": {
        "id": "uJmnXF6kUYgi"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configs = {\"unit\" : [128,256,384,384,384,512,512,512],\n",
        "           \"kernel_size\" : [7,5,5,3,3,3,3,3],\n",
        "           \"stride\" : [3,1,1,1,1,1,1,1],\n",
        "           \"padding\" : [4,1,1,1,1,1,1,1]}\n",
        "layer_list = [\"conv2d\", \"conv2d\", \"conv2d\" \"maxpool\", \"conv2d\", \"conv2d\", \"maxpool\", \"conv2d\", \"conv2d\", \"conv2d\", \"maxpool\"]"
      ],
      "metadata": {
        "id": "pXsPJljnUbYY"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getCNN(configs, layer_list):\n",
        "    layers = []\n",
        "    in_channels = 1\n",
        "    i = 0\n",
        "    for layer in layer_list:\n",
        "        if layer == \"conv2d\":\n",
        "            layers.append(nn.Conv2d(in_channels, configs[\"unit\"][i], kernel_size = configs[\"kernel_size\"][i], stride = configs[\"stride\"][i], padding = configs[\"padding\"][i]))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            in_channels = configs[\"unit\"][i]\n",
        "            i += 1\n",
        "        elif layer == \"maxpool\":\n",
        "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "    model = nn.Sequential(*layers)\n",
        "    return model"
      ],
      "metadata": {
        "id": "qDM3ihK3Uc7z"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getFCN():\n",
        "  layers = []\n",
        "  layers.append(nn.Linear(512*9*9, 4096))\n",
        "  layers.append(nn.ReLU(True))\n",
        "  layers.append(nn.Dropout(p=0))\n",
        "  layers.append(nn.Linear(4096, 4096))\n",
        "  layers.append(nn.ReLU(True))\n",
        "  layers.append(nn.Dropout(p=0))\n",
        "  layers.append(nn.Linear(4096, 512))\n",
        "  layers.append(nn.Tanh())\n",
        "  model = nn.Sequential(*layers)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "LapmTvODUep0"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Img2MolModel(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.learning_rate = LR\n",
        "        self.features = getCNN(configs, layer_list)\n",
        "        self.classifier = getFCN()\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = F.mse_loss(y_hat, y)\n",
        "        self.log('train_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = F.mse_loss(y_hat, y)\n",
        "        self.log('valid_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = F.mse_loss(y_hat, y)\n",
        "        self.log('test_loss', loss)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)"
      ],
      "metadata": {
        "id": "KaWbpp77Uf-P"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Img2MolModel()"
      ],
      "metadata": {
        "id": "1gFYa5Z0Uhdy"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive"
      ],
      "metadata": {
        "id": "wXjPYPzWVHTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = sorted(os.listdir(\"train_data\"))"
      ],
      "metadata": {
        "id": "vyiEqbt9Ul4J"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_names = []\n",
        "for train_image in train_images[:5000]:\n",
        "    image_names.append(os.path.join(\"train_data\",train_image))"
      ],
      "metadata": {
        "id": "3DIcWeMIVEsf"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(path):\n",
        "    im = Image.open(path).convert('RGB')\n",
        "    im = ImageEnhance.Contrast(ImageOps.autocontrast(im)).enhance(2.0)\n",
        "    im = ImageOps.autocontrast(im)\n",
        "    im = im.resize((234,234))\n",
        "    im = np.array(im)\n",
        "    im = transforms.ToTensor()(im)\n",
        "    return im"
      ],
      "metadata": {
        "id": "RHeC_FPZYeG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = torch.unsqueeze(load_image(image_names[0]), 0)\n",
        "for image_name in tqdm(image_names):\n",
        "  images = torch.cat((images, torch.unsqueeze(load_image(image_name), 0)), dim = 0)"
      ],
      "metadata": {
        "id": "AAGrmwISasnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "chembl = pd.read_csv(\"chembl_cleaned.csv\", delimiter = \"\\t\")\n",
        "chembl = chembl[:50000]\n",
        "smiles = chembl[\"Smiles\"]\n",
        "smiles = list(smiles)"
      ],
      "metadata": {
        "id": "DBbLcjVS3IH_"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "requests.packages.urllib3.disable_warnings()\n",
        "DEFAULT_HOST = \"http://ec2-18-157-240-87.eu-central-1.compute.amazonaws.com\"\n",
        "class CDDDRequest:\n",
        "    def __init__(self, host=DEFAULT_HOST, port=8892):\n",
        "        self.host = host\n",
        "        self.port = port\n",
        "        self.headers = {'content-type': 'application/json'}\n",
        "\n",
        "    def smiles_to_cddd(self, smiles):\n",
        "        url = \"{}:{}/smiles_to_cddd/\".format(self.host, self.port)\n",
        "        req = json.dumps({\"smiles\": smiles})\n",
        "        response = requests.post(url, data=req, headers=self.headers, verify=False)\n",
        "        return json.loads(response.content.decode(\"utf-8\"))"
      ],
      "metadata": {
        "id": "SmdGD-V32Nsl"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CDDDserver = CDDDRequest()\n",
        "cddd = []\n",
        "for smile in tqdm(smiles):\n",
        "  cddd.append(CDDDserver.smiles_to_cddd(smile))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEewNGdk2wRV",
        "outputId": "40d8d6ef-d1ed-460d-b484-4ee15bbfde2a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [17:31<00:00,  1.05s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "ASx7C_nG8XAG"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cddd = np.array(cddd)\n",
        "cddd = torch.from_numpy(cddd).to(device)"
      ],
      "metadata": {
        "id": "xHxktIw59p4M"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = images[1:]"
      ],
      "metadata": {
        "id": "mFmjE1Y--4wr"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "for i in range(len(images)):\n",
        "  dataset.append([images[i], cddd[i]])\n",
        "train_size = int(0.8 * len(images))\n",
        "test_size = len(images) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [int(train_size*0.9), int(train_size*0.1)])"
      ],
      "metadata": {
        "id": "qpbvxj5u-kcQ"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=128)\n",
        "valloader = torch.utils.data.DataLoader(val_dataset, batch_size = 128)\n",
        "trainer = pl.Trainer(callbacks=[pl.callbacks.TQDMProgressBar(refresh_rate=10)])\n",
        "trainer.fit(model, trainloader, valloader)\n",
        "torch.save(model.save_dict(), \"model.ckpt\")"
      ],
      "metadata": {
        "id": "CSJtJyy1YvF5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GMms2aN1_VO0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}